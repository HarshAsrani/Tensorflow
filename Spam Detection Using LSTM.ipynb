{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import GlobalMaxPooling1D, Dense, Input, LSTM, Embedding\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('spam.csv', encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.columns = ['labels','data']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['b_labels'] = df['labels'].map({'ham':0, 'spam':1})\nY = df['b_labels'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train, df_test, Y_train, Y_test = train_test_split(df['data'], Y, test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"MAX_VOCAB_SIZE = 20000\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(df_train)\nsequences_train = tokenizer.texts_to_sequences(df_train)\nsequences_test = tokenizer.texts_to_sequences(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"word2idx = tokenizer.word_index\nV = len(word2idx)\nprint('Found %s unique tokens' %V)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_train = pad_sequences(sequences_train)\nprint(data_train.shape)\nT = data_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_test = pad_sequences(sequences_test, maxlen=T)\ndata_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"D = 20    #Embedding Dimensionality\nM = 15    #Hidden state Dimensionality\n\ni = Input(shape=(T,))\nx = Embedding(V + 1, D)(i)\nx = LSTM(M, return_sequences=True)(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i,x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nr = model.fit(data_train, Y_train, epochs=10, validation_data=(data_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(r.history['accuracy'], label='accuracy')\nplt.plot(r.history['val_accuracy'], label='val_accuracy')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}